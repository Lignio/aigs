For this lab I have learned how to implement Monte-Carlo tree search. The implementation was based on the pseudocode provided in class. MCTS is useful for exploring vast search spaces were sampling part of the state-space without having to map the entire thing is necessary due to size constraints. MCTS has multiple optimizations that can be implemented, but I decided to just implement a basic version for this lab, to get an overall idea of how that feels. Immediate improvements that could be made would be saving the tree, so it isn’t entirely remade every turn, especially saving it between games would massively improve the current algorithm’s ability to actually win. Currently the tree’s expansion policy is also very basic, and making it focus a bit more on depth rather than almost entirely breadth at each level would likely create better results. 